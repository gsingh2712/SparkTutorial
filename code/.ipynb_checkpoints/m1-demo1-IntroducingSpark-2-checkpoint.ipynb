{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data = sc.parallelize([1, \"Alice\", 50])\n",
    "simple_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.1\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_191\"\r\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_191-b12)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Alice']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Alice', 50]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-93ca8d20898d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## fails becuase data types are different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-install/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-install/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-install/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-install/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-install/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'int'>"
     ]
    }
   ],
   "source": [
    "df = simple_data.toDF() ## fails becuase data types are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[8] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = sc.parallelize([[1, \"Alice\", 50], [2, \"Bob\", 80]])\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Alice', 50], [2, 'Bob', 80]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Alice', 50]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string, _3: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = records.toDF()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| _1|   _2| _3|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 50|\n",
      "|  2|  Bob| 80|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Row if you want to associate column names with RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Alice', score=50),\n",
       " Row(id=2, name='Bob', score=80),\n",
       " Row(id=3, name='Charless', score=75)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row means single record\n",
    "data = sc.parallelize([Row(id=1,name=\"Alice\",score=50),\n",
    "                      Row(id=2,name=\"Bob\",score=80),\n",
    "                      Row(id=3,name=\"Charless\",score=75)])\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    name|score|\n",
      "+---+--------+-----+\n",
      "|  1|   Alice|   50|\n",
      "|  2|     Bob|   80|\n",
      "|  3|Charless|   75|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-------------------+\n",
      "|            col_dict|            col_list|     col_row|           col_time|\n",
      "+--------------------+--------------------+------------+-------------------+\n",
      "|           [k1 -> 0]|           [1, 2, 3]|[10, 20, 30]|2014-08-01 14:01:05|\n",
      "|  [k1 -> 0, k2 -> 1]|     [1, 2, 3, 4, 5]|[10, 20, 30]|2014-08-02 14:01:06|\n",
      "|[k3 -> 2, k1 -> 0...|[1, 2, 3, 4, 5, 6...|[10, 20, 30]|2014-08-03 14:01:07|\n",
      "+--------------------+--------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### complex data Row supports all sorts of Data Dict as column\n",
    "### Even nested row as column \n",
    "### Datetime as column\n",
    "### List a clumn\n",
    "complex_data= sc.parallelize([Row(\n",
    "                             col_list = [1,2,3],\n",
    "                             col_dict = {'k1':0},\n",
    "                             col_row = Row(a=10, b=20 , c=30),\n",
    "                             col_time = datetime(2014, 8, 1 , 14, 1 , 5)\n",
    "                            ),\n",
    "                            Row(\n",
    "                             col_list = [1,2,3,4,5],\n",
    "                             col_dict = {'k1':0, 'k2':1},\n",
    "                             col_row = Row(a=10, b=20 , c=30),\n",
    "                             col_time = datetime(2014, 8, 2 , 14, 1 , 6)\n",
    "                            ),\n",
    "                             Row(\n",
    "                             col_list = [1,2,3,4,5,6,7],\n",
    "                             col_dict = {'k1':0, 'k2':1, 'k3':2 },\n",
    "                             col_row = Row(a=10, b=20 , c=30),\n",
    "                             col_time = datetime(2014, 8, 3 , 14, 1 , 7)\n",
    "                            )])\n",
    "df=complex_data.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x117ed8860>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.range(5)\n",
    "df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating DataFrames directly from input Data, no conversion from RDD to DF\n",
    "data = [('Alice', 50),\n",
    "        ('Bob', 80),\n",
    "        ('Charleee', 75)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DF from SQLContext is easier , no need for RDD\n",
    "## Creating DF from Data Directly , using createDataDrame Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|      _1| _2|\n",
      "+--------+---+\n",
      "|   Alice| 50|\n",
      "|     Bob| 80|\n",
      "|Charleee| 75|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    Name|Score|\n",
      "+--------+-----+\n",
      "|   Alice|   50|\n",
      "|     Bob|   80|\n",
      "|Charleee|   75|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(data,['Name', 'Score']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_data = [\n",
    "                (1.0,\n",
    "                10,\n",
    "                \"Alice\",\n",
    "                 True,\n",
    "                [1,2,3],\n",
    "                {\"k1\": 0},\n",
    "                Row(a=1, b=2, c=3),\n",
    "                datetime(2014, 8, 3 , 14, 1 , 5)),\n",
    "                \n",
    "                (2.0,\n",
    "                20,\n",
    "                \"Bob\",\n",
    "                 True,\n",
    "                [1,2,3],\n",
    "                {\"k1\": 0,\"k2\":1},\n",
    "                Row(a=1, b=2, c=3),\n",
    "                datetime(2014, 8, 3 , 14, 1 , 6)),\n",
    "    \n",
    "                (3.0,\n",
    "                30,\n",
    "                \"Charlee\",\n",
    "                 False,\n",
    "                [1,2,3],\n",
    "                {\"k1\": 0, \"k2\":1 , \"k3\":1},\n",
    "                Row(a=1, b=2, c=3),\n",
    "                datetime(2014, 8, 3 , 14, 1 , 7))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "|col_integer|col_float|col_string|col_boolean| col_list|      col_dictionary|  col_row|      col_date_time|\n",
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "|        1.0|       10|     Alice|       true|[1, 2, 3]|           [k1 -> 0]|[1, 2, 3]|2014-08-03 14:01:05|\n",
      "|        2.0|       20|       Bob|       true|[1, 2, 3]|  [k1 -> 0, k2 -> 1]|[1, 2, 3]|2014-08-03 14:01:06|\n",
      "|        3.0|       30|   Charlee|      false|[1, 2, 3]|[k3 -> 1, k1 -> 0...|[1, 2, 3]|2014-08-03 14:01:07|\n",
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df = sqlContext.createDataFrame(complex_data,\n",
    "                                            ['col_integer',\n",
    "                                             'col_float',\n",
    "                                             'col_string',\n",
    "                                             'col_boolean',\n",
    "                                             'col_list',\n",
    "                                             'col_dictionary',\n",
    "                                             'col_row',\n",
    "                                             'col_date_time'])\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of specifying Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.parallelize([\n",
    "    Row(1, \"Alice\", 50),\n",
    "    Row(2, \"Bob\" , 80),\n",
    "    Row(3, \"Charlee\", 75)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giving Column names later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a map operation on each RDD to\n",
    "### assign name to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names= Row('id', 'name' , 'score')\n",
    "students = data.map(lambda r: column_names(*r)) ## map operates on every element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Alice', score=50),\n",
       " Row(id=2, name='Bob', score=80),\n",
       " Row(id=3, name='Charlee', score=75)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students.collect()### feild names assigned to row object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, name: string, score: bigint]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### creating a DataFrame for RDD\n",
    "students_df = sqlContext.createDataFrame(students)\n",
    "students_df ## Notice type of each column is infered correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Complex DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(col_integer=1.0, col_float=10, col_string='Alice', col_boolean=True, col_list=[1, 2, 3], col_dictionary={'k1': 0}, col_row=Row(a=1, b=2, c=3), col_date_time=datetime.datetime(2014, 8, 3, 14, 1, 5))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DF is a tabular form and you access it as individual cells\n",
    "cell_string = complex_data_df.collect()[0][2]\n",
    "cell_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(col_integer=1.0, col_float=10, col_string='Alice', col_boolean=True, col_list=[1, 2, 3], col_dictionary={'k1': 0}, col_row=Row(a=1, b=2, c=3), col_date_time=datetime.datetime(2014, 8, 3, 14, 1, 5))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_list = complex_data_df.collect()[0][4] ## returns a copy not actual \n",
    "print(cell_list)\n",
    "## modifying list doesn't affect DF\n",
    "cell_list.append(100)\n",
    "complex_data_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every DataFrame Stores the RDD equivalent of the records in the variable rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', {'k1': 0}),\n",
       " ('Bob', {'k1': 0, 'k2': 1}),\n",
       " ('Charlee', {'k3': 1, 'k1': 0, 'k2': 1})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df.rdd\\\n",
    "               .map(lambda x: (x.col_string , x.col_dictionary))\\\n",
    "               .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------------+\n",
      "|col_string| col_list|      col_date_time|\n",
      "+----------+---------+-------------------+\n",
      "|     Alice|[1, 2, 3]|2014-08-03 14:01:05|\n",
      "|       Bob|[1, 2, 3]|2014-08-03 14:01:06|\n",
      "|   Charlee|[1, 2, 3]|2014-08-03 14:01:07|\n",
      "+----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Getting column using select \n",
    "complex_data_df.select('col_string','col_list','col_date_time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|col_integer|col_float|col_sum|\n",
      "+-----------+---------+-------+\n",
      "|        1.0|       10|   11.0|\n",
      "|        2.0|       20|   22.0|\n",
      "|        3.0|       30|   33.0|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Adding a new column to DF, Some things you can do using select\n",
    "## This returns a Data Frame\n",
    "complex_data_df.select('col_integer','col_float')\\\n",
    "                .withColumn(\"col_sum\", complex_data_df.col_integer + complex_data_df.col_float)\\\n",
    "                .show()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following operations create new DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "|col_integer|col_float|col_string|col_boolean| col_list|             col_map|  col_row|      col_date_time|\n",
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "|        1.0|       10|     Alice|       true|[1, 2, 3]|           [k1 -> 0]|[1, 2, 3]|2014-08-03 14:01:05|\n",
      "|        2.0|       20|       Bob|       true|[1, 2, 3]|  [k1 -> 0, k2 -> 1]|[1, 2, 3]|2014-08-03 14:01:06|\n",
      "|        3.0|       30|   Charlee|      false|[1, 2, 3]|[k3 -> 1, k1 -> 0...|[1, 2, 3]|2014-08-03 14:01:07|\n",
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.withColumnRenamed('col_dictionary', 'col_map').show() ## New DF with renamed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "|Charlee|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.select(complex_data_df.col_string.alias(\"Name\")).show() ## new 1 Column DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DF can be converted to Pandas DF\n",
    "## Spark DF are built on RDD and distributed across multiple nodes\n",
    "## Where as Pandas DF are in memory on a single machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_integer</th>\n",
       "      <th>col_float</th>\n",
       "      <th>col_string</th>\n",
       "      <th>col_boolean</th>\n",
       "      <th>col_list</th>\n",
       "      <th>col_dictionary</th>\n",
       "      <th>col_row</th>\n",
       "      <th>col_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Alice</td>\n",
       "      <td>True</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'k1': 0}</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>2014-08-03 14:01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>Bob</td>\n",
       "      <td>True</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'k1': 0, 'k2': 1}</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>2014-08-03 14:01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>30</td>\n",
       "      <td>Charlee</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>{'k3': 1, 'k1': 0, 'k2': 1}</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>2014-08-03 14:01:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_integer  col_float col_string  col_boolean   col_list  \\\n",
       "0          1.0         10      Alice         True  [1, 2, 3]   \n",
       "1          2.0         20        Bob         True  [1, 2, 3]   \n",
       "2          3.0         30    Charlee        False  [1, 2, 3]   \n",
       "\n",
       "                col_dictionary    col_row       col_date_time  \n",
       "0                    {'k1': 0}  (1, 2, 3) 2014-08-03 14:01:05  \n",
       "1           {'k1': 0, 'k2': 1}  (1, 2, 3) 2014-08-03 14:01:06  \n",
       "2  {'k3': 1, 'k1': 0, 'k2': 1}  (1, 2, 3) 2014-08-03 14:01:07  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = complex_data_df.toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark DF from Pandas DF using SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "|col_integer|col_float|col_string|col_boolean| col_list|      col_dictionary|  col_row|      col_date_time|\n",
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "|        1.0|       10|     Alice|       true|[1, 2, 3]|           [k1 -> 0]|[1, 2, 3]|2014-08-03 14:01:05|\n",
      "|        2.0|       20|       Bob|       true|[1, 2, 3]|  [k1 -> 0, k2 -> 1]|[1, 2, 3]|2014-08-03 14:01:06|\n",
      "|        3.0|       30|   Charlee|      false|[1, 2, 3]|[k3 -> 1, k1 -> 0...|[1, 2, 3]|2014-08-03 14:01:07|\n",
      "+-----------+---------+----------+-----------+---------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = sqlContext.createDataFrame(df_pandas).show()\n",
    "df_spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
